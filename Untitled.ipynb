{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import IOStream, set_gpu, ensure_path, Timer, count_acc,  Averager, compute_confidence_interval, euclidean_metric\n",
    "from dataset.dataset import MiniImagenet\n",
    "from dataset.sampler import CategoriesSampler\n",
    "from model.protonet import ProtoNet\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='mini_imagenet', distance='l2', gamma=0.2, gpu=0, init_weights=None, lr=0.0001, max_epoch=200, model_type='ConvNet', n_batch=100, num_tasks=5, query=5, save_path='./MINI_ProtoNet_MINI_1shot_5way', shot=1, step_size=10, way=5)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='mini_imagenet')\n",
    "parser.add_argument('--distance', default='l2')\n",
    "parser.add_argument('--shot', default=1, type=int)\n",
    "parser.add_argument('--way', default=5, type=int)\n",
    "parser.add_argument('--query', default=5, type=int)\n",
    "parser.add_argument('--num_tasks', default=5, type=int)\n",
    "parser.add_argument('--n_batch', default=100, type=int)\n",
    "parser.add_argument('--max_epoch', default=200, type=int)\n",
    "\n",
    "parser.add_argument('--gpu', default=0)\n",
    "# optimizer\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--step_size', type=int, default=10)  # step size and gamma use to lr scheduler\n",
    "parser.add_argument('--gamma', type=float, default=0.2)\n",
    "# model\n",
    "parser.add_argument('--init_weights', type=str, default=None)\n",
    "parser.add_argument('--model_type', default='ConvNet', type=str, choices=['ConvNet', 'ResNet', 'AmdimNet'])\n",
    "# io\n",
    "parser.add_argument('--save_path', type=str, default='./MINI_ProtoNet_MINI_1shot_5way')\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "io = IOStream('log/run.log')\n",
    "io.cprint(str(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDir(dirPath):\n",
    "    if not os.path.isdir(dirPath):\n",
    "        return\n",
    "    files = os.listdir(dirPath)\n",
    "    for file in files:\n",
    "        filePath = os.path.join(dirPath,file)\n",
    "        if os.path.isfile(filePath):\n",
    "            os.remove(filePath)\n",
    "        elif os.path.isdir(filePath):\n",
    "            removeDir(filePath)\n",
    "    os.rmdir(dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removeDir('MINI_ProtoNet_MINI_1shot_5way/mini_imagenet-ConvNet-ProtoNet/')\n",
    "# os.rmdir('MINI_ProtoNet_MINI_1shot_5way/mini_imagenet-ConvNet-ProtoNet/1_5_5/summary')\n",
    "# osp.exists('MINI_ProtoNet_MINI_1shot_5way/mini_imagenet-ConvNet-ProtoNet/1_5_5/summary')\n",
    "# os.mkdir('MINI_ProtoNet_MINI_1shot_5way/mini_imagenet-ConvNet-ProtoNet/1_5_5/summary')\n",
    "# shutil.rmtree('MINI_ProtoNet_MINI_1shot_5way/mini_imagenet-ConvNet-ProtoNet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset = MiniImagenet('val', args)\n",
    "len(valset.data)  # 9600\n",
    "val_sampler = CategoriesSampler(valset.label, args.n_batch, args.way * args.num_tasks, args.shot + args.query)\n",
    "len(val_sampler.m_ind)   # 16\n",
    "val_sampler.m_ind[0].shape  # 600\n",
    "# for i in val_sampler:\n",
    "#     print(i.shape)  # torch.Size([96])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "len(trainset.data)   #38400\n",
    "for i in train_sampler:\n",
    "    print(i.shape)   # torch.Size([150])  6*25\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu: 0\n"
     ]
    }
   ],
   "source": [
    "set_gpu(args.gpu)\n",
    "save_path1 = '-'.join([args.dataset, args.model_type, 'ProtoNet'])\n",
    "save_path2 = '_'.join([str(args.shot), str(args.query), str(args.way)])\n",
    "args.save_path = osp.join(args.save_path, osp.join(save_path1, save_path2))\n",
    "\n",
    "ensure_path(save_path1, remove=False)\n",
    "ensure_path(args.save_path)  \n",
    "\n",
    "trainset = MiniImagenet('train', args)\n",
    "train_sampler = CategoriesSampler(trainset.label, args.n_batch, args.way * args.num_tasks, args.shot + args.query)\n",
    "train_loader = DataLoader(dataset=trainset, batch_sampler=train_sampler, num_workers=2, pin_memory=True)\n",
    "\n",
    "# only 16 classes in val, so no num_tasks\n",
    "valset = MiniImagenet('val', args)   \n",
    "val_sampler = CategoriesSampler(valset.label, args.n_batch, args.way, args.shot + args.query)  \n",
    "val_loader = DataLoader(dataset=valset, batch_sampler=val_sampler, num_workers=2, pin_memory=True)\n",
    "\n",
    "model = ProtoNet(args)\n",
    "if args.model_type == 'ConvNet':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "elif args.model_type == 'ResNet':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=True, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)        \n",
    "\n",
    "        # logits = model(data_shot, data_query)\n",
    "        # loss = F.cross_entropy(logits, label)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "# load pretrained model initialization, according to my test, it can't work\n",
    "if args.init_weights is not None:\n",
    "    model_detail = torch.load(args.init_weights)\n",
    "    if 'params' in model_detail:\n",
    "        pretrained_dict = model_detail['params']\n",
    "        # remove weights for FC\n",
    "        pretrained_dict = {'encoder.'+k: v for k, v in pretrained_dict.items()}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "    else:\n",
    "        pretrained_dict = model_detail['model']\n",
    "        pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items() if k.replace('module.', '') in model_dict}\n",
    "        # pretrained_dict is empty\n",
    "        model_dict.update(pretrained_dict)           \n",
    "\n",
    "model.load_state_dict(model_dict)    \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log component\n",
    "trlog = {}\n",
    "trlog['args'] = vars(args)\n",
    "trlog['train_loss'] = []\n",
    "trlog['val_loss'] = []\n",
    "trlog['train_acc'] = []\n",
    "trlog['val_acc'] = []\n",
    "trlog['max_acc'] = 0.0\n",
    "trlog['max_acc_epoch'] = 0\n",
    "\n",
    "timer = Timer()\n",
    "global_count = 0\n",
    "writer = SummaryWriter(logdir=osp.join(args.save_path, 'summary'))\n",
    "\n",
    "\n",
    "for epoch in range(1, args.max_epoch + 1):\n",
    "    lr_scheduler.step()\n",
    "    model.train()\n",
    "    tl = Averager()  # average train loss of the epoch\n",
    "    ta = Averager()  # train acc of the epoch\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        global_count = global_count + 1\n",
    "        data, _ = [_.cuda() for _ in batch]\n",
    "        p = args.num_tasks * args.shot * args.way\n",
    "        data_shot, data_query = data[:p], data[p:]\n",
    "\n",
    "        label = torch.arange(args.way*args.num_tasks).repeat(args.query)\n",
    "        if torch.cuda.is_available():\n",
    "            label = label.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            label = label.type(torch.LongTensor)\n",
    "            \n",
    "        logits = model(data_shot, data_query)\n",
    "\n",
    "        \n",
    "        \n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 3, 84, 84])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "label.shape  # torch.Size([125])\n",
    "logits.shape  # torch.Size([125, 25])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 25, -1]' is invalid for input of size 320",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-3790360ca063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdata_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#         loss = F.cross_entropy(logits, label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/test/5Mycode/ProtoNets/model/protonet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_shot, data_query)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# torch.Size([5, 64])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mway\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# proto:(task*n_way, feat); query()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# torch.Size([5, 64])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 25, -1]' is invalid for input of size 320"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  \n",
    "    for i, batch in enumerate(val_loader, 1):\n",
    "        if torch.cuda.is_available():\n",
    "            data, _ = [_.cuda() for _ in batch]\n",
    "        else:\n",
    "            data = batch[0]\n",
    "\n",
    "        label = torch.arange(args.way).repeat(args.query)\n",
    "        if torch.cuda.is_available():\n",
    "            label = label.type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            label = label.type(torch.LongTensor)\n",
    "\n",
    "        p = args.shot * args.way\n",
    "        data_shot, data_query = data[:p], data[p:]\n",
    "\n",
    "        logits = model(data_shot, data_query)\n",
    "#         loss = F.cross_entropy(logits, label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 84, 84])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val\n",
    "label.shape   # torch.Size([25])\n",
    "logits.shape  # torch.Size([71, 25])\n",
    "data_query.shape  torch.Size([25, 3, 84, 84])\n",
    "# p    # 5\n",
    "# data.shape   # ([30, 3, 84, 84]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(range(24)).view(2,3,4)\n",
    "a.shape\n",
    "b = torch.tensor(range(24, 48, 1)).view(2,3,4)\n",
    "logits = -((a - b)**2).sum(dim=2)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4281, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)  # tensor([1, 3, 4])\n",
    "loss = F.cross_entropy(ip, target)\n",
    "loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch041-gpu",
   "language": "python",
   "name": "torch041-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
